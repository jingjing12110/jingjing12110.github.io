---
title: '网络训练和设计技巧'
date: 2019-06-28
permalink: /posts/2019/06/网络训练和设计技巧/
tags:
  - 模型调试
  - 深度学习
---

总结深度学习网络训练和调试的技巧。

## Neural Networks and deep learning -- by Michael

### 实验结果糟糕?

- 使用了让网络难以学习的初始化（w, b）权重和偏置；
- 训练数据太少，不足以使网络获得有意义的学习；
- 没有进行足够的迭代期(epoch);
- 学习速率太低或者太高；
- 该神经网络的结构不适合该任务；

注： 调试⼀个神经⽹络不是琐碎的，就像常规编程那样，它是⼀⻔艺术。你需要学习调试
的艺术来获得神经⽹络更好的结果。更普通的是，我们需要启发式⽅法来选择好的超参数和
好的结构。

### 好的实验结果意味着什么？和什么相比算好？

Baseline Method

### 改进神经网络的学习方法？

- 更好的代价函数的选择-交叉熵代价函数；

- “规范化”的方法；
- 更好的权重初始化方法；
- 帮助选择好的超参数的启发式想法；
- 其他；

### 深度神经网络为何难训练？

- 梯度消失问题

### 1x1 卷积的意义

- 能够完成特征图通道的聚合或发散
- 改变(减少)卷积参数数量

---

## 实践经验

### batch size 的选择？

- 在合理范围内，增大 Batch_Size 有何好处？

  - 内存利用率提高了，大矩阵乘法的并行化效率提高;
  - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步
    加快；
  - 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越
    小；

- 盲目增大 Batch_Size 有何坏处？
  - 内存利用率提高了，但是内存容量可能撑不住了；
  - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢；

注： 在开始训练是尝试较小的 batch size， 如 16， 8, 1？？？对吗？？？

### 训练震荡？

- 调大 batch size 相对于正常数据集，如果 Batch_Size 过小，训练数据就会非常难收敛，从而导致 underfitting。

- 调小学习率, 学习速率设置过小，会极大降低收敛速度，增加训练时间；
- 学习率太大，可能导致参数在最优解两侧来回振荡。
  ![loss-epoch](/images/blog/loss-epoch.jpeg)

图片分析：

1. 曲线初始时上扬 [红线]： Solution：初始学习率过大导致振荡，应减小学习率，并从头开始训练。
1. 曲线初始时强势下降没多久归于水平[紫线]： Solution：后期学习率过大导致无法拟合，应减小学习率，并重新训练后几轮。
1. 曲线全程缓慢[黄线]：Solution：初始 学习率过小导致收敛慢，应增大学习率，并从头开始训练 。

### 过拟合？

训练集损失**远远小于**验证集损失；所谓远远小于是指训练集损失和验证集损失不在同一个数量级，而若训练集损失只比验证集损失多一点点(同一数量级如 0.8、0.9)的话并不是过拟合现象。

- 增大训练数据平移、旋转、加噪、亮度改变、饱和度改变-取决于数据本身
- 采用正则化方法在 pytorch 中是 weight_decay 这个参数，默认使用的是 L2 正则化，可以从 1e-4 进行尝试。
- Dropout 简单来说就是在神经网络模型进行前向传播的过程中，随机选取和丢弃指定层次之间的部分神经连接，因为整个过程是随机的，所以能有效防止过拟合的发生。

### 优化算法的选择？

**理解数据对于设计算法的必要性。** 先用 Adam 进行快速下降，而后再换到 SGD 进行充分的调优。

### 学习率的选择？

[fastai](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html) 中lr_find()函数可以帮助寻找合适的学习率, 但fastai的model为内置的模型。

- 一般来说，越大的 batch-size 使用越大的学习率，

### loss 不下降

#### train loss 不下降

- model设计不合适
- 权重初始化方法不对
- 正则化过度
- 选择合适的激活函数、损失函数
  卷积神经网络中，卷积层的输出，一般使用ReLu作为激活函数，因为可以有效避免梯度消失，并且线性函数在计算性能上面更加有优势。而循环神经网络中的循环层一般为tanh，或者ReLu，全连接层也多用ReLu，只有在神经网络的输出层，使用全连接层来分类的情况下，才会使用softmax这种激活函数。
  而损失函数，对于一些分类任务，通常使用交叉熵损失函数，回归任务使用均方误差，有自动对齐的任务使用CTC loss等。损失函数相当于模型拟合程度的一个评价指标，这个指标的结果越小越好。一个好的损失函数，可以在神经网络优化时，产生更好的模型参数。
- 选择合适的优化器和学习速率
- 训练时间不足
- 模型训练遇到瓶颈
  梯度消失、大量神经元失活、梯度爆炸和弥散、学习率过大或过小等。
- batch size过大
  **batch size过小，会导致模型后期摇摆不定，迟迟难以收敛，而过大时，模型前期由于梯度的平均，导致收敛速度过慢**。一般batch size 的大小常常选取为32，或者16，有些任务下比如NLP中，可以选取8作为一批数据的个数。
- 数据集未打乱
- 数据集有问题
  当一个数据集噪声过多，或者数据标注有大量错误时，会使得神经网络难以从中学到有用的信息，从而出现摇摆不定的情况。
- 未进行归一化

#### val loss 不下降

验证集的loss不下降分为两种。一种是训练集上的loss也不下降，这时问题主要在训练集的loss上，另一种是训练集上的loss可以下降，但验证集上的loss已经不降了。

- 适当的正则化和降维
- 适当降低模型的规模
- 获取更多的数据集
- 对数据集做扰动和扩增

#### loss下降，acc不提高

- 增大lr，从新训练

#### train loss 在每个epoch开始时跳变，波动很大，随后趋于平缓，不下降

- lr 太小

### 网络调试

网络学习效果很差（网络在训练中的损失/准确率不收敛）时：  
  
  1. 如果网络学习效果不佳，首先应该做的就是去过拟合一个训练数据点。准确率基本上应该达到 100% 或 99.99%，或者说误差接近 0。如果神经网络不能对一个数据点达到过拟合，那么模型架构就可能存在很严重的问题，但这种问题可能是十分细微的。如果可以过拟合一个数据点，但是在更大的集合上训练时仍然不能收敛，则尝试以下方法。
  2. 降低学习率。网络会学习地更慢，但是它可能会找到一个之前使用较大的步长时没找到的最小值。
  3. 提高学习率。这将加快训练速度，有助于加强反馈回路（feedback loop）。这意味着很快就能大概知道网络是否有效。尽管这样一来网络应该能更快地收敛，但是训练结果可能不会太好，而且这种「收敛」状态可能实际上是反复震荡的。
  4. 减小（小）批量处理的规模。将批处理大小减小到 1 可以提供与权重更新相关的更细粒度的反馈。
  5. 删掉批归一化层。在将批处理大小减小为 1 时，这样做会暴露是否有梯度消失和梯度爆炸等问题。
  6. 检查矩阵的重构「reshape」。大幅度的矩阵重构（比如改变图像的 X、Y 维度）会破坏空间局部性，使网络更不容易学习，因为这时网络也必须学习重构。（自然特征变得支离破碎。事实上自然特征呈现出空间局部性也是卷积神经网络能够如此有效的原因！）使用多个图像/通道进行重构时要特别小心；可以使用 numpy.stack() 进行适当的对齐操作。
  7. 仔细检查损失函数。如果使用的是一个复杂的函数，可以试着把它简化为 L1 或 L2 这样的形式。发现 L1 对异常值不那么敏感，当遇到带有噪声的批或训练点时，可以进行稍小幅度的调整。

---

## 理论学习

### [深度学习中的优化算法](https://zhuanlan.zhihu.com/p/32626442)

<center>
<img src="/images/blog/GD.png" width="90%" height="90%"/>

GD 一般过程

</center>

1. SGD(Stochastic Gradient Descent)-改进更新参数的过程 **vanilla SGD**, 朴素
   SGD，最简单的随机梯度下降算法/批梯度下降算法，没有动量的概念；缺点是收敛速度
   慢，可能在鞍点处震荡，如何选学习率是一大难点；
   `torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)。` SGD 在遇到沟壑
   时容易陷入震荡，为此，可以为其引入动量**Momentum**，加速 SGD 在正确方向的下降
   并抑制震荡，**SGD-M**; 更进一步的，人们希望下降的过程更加智能：算法能够在目标
   函数有增高趋势之前，减缓更新速率，**NAG**(Nesterov Accelerated Gradient)即是
   为此而设计的，其在 SGD-M 的基础上进一步改进了梯度计算公式; SGD、SGD-M 和 NAG
   均是**以相同的学习率去更新$\theta$的各个分量**,对于更新不频繁的参数我们希望单
   次步长更大，多学习一些知识；对于更新频繁的参数，我们则希望步长较小，使得学习
   到的参数更稳定，不至于被单个样本影响太多。
2. Adam()-优化学习率 **Adagrad**算法即可达到此效果,引入二阶动量；在 Adagrad 中
   ，$v_t$是单调递增的，使得学习率逐渐递减至 0，可能导致训练过程提前结束。为了改
   进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间
   窗口内的下降梯度，**RMSprop**; **Adam**可以认为是 RMSprop 和 Momentum 的结合
   ，和 RMSprop 对二阶动量使用指数移动平均类似，Adam 中对一阶动量也是用指数移动
   平均计算； **NAdam**在 Adam 之上融合了 NAG 的思想；

### [BatchNormalize](https://arxiv.org/abs/1502.03167)

数据预处理望达到：**0均值、单位方差、弱相关性**等要求。
在每一层的激活函数之后，例如$ReLU=\max(Wx+b,0)$之后，应当对数据进行归一化。然而，文章中说这样做在训练初期，分界面还在剧烈变化时， 计算出的参数不稳定，所以退而求其次，在$Wx+b$之后进行归一化。因为初始的W是从标准高斯分布中采样得到的，而W中元素的数量远大于x， Wx+b每维的均值本身就接近0、方差接近1，所以在Wx+b后使用Batch Normalization能得到更稳定的结果。

normal分布变换成随机分布?
GAN需要加BN层吗?

### 网络优化与正则化

神经网络具有非常强的表达能力, 但应用神经网络到机器学习时,存在两大难点:
- 优化问题
神经网络模型是一个**非凸函数**，再加上在深度网络中的**梯度消失问题**，很难进行优化；另外，深层神经网络模型一般参数比较多，训练数据也比较大，会导致训练的效率比较低;
- 泛化问题
因为神经网络的拟合能力强，反而容易在训练集上产生过拟合。因此，在训练深层神经网络时，同时也需要通过一定的**正则化方法来改进网络的泛化能力**.

#### 网络优化

**网络优化的难点**:

- 网络结构多样性: 由于网络结构的多样性, 很难找到一种通用的优化方法, 不同的优化方法在不同网络结构上的差异也都比较大; 网络的超参数一般也比较多, 给优化带来挑战.
- 高维变量的非凸优化: 在高维空间中，非凸优化的难点并不在于如何逃离局部最优点，而是如何逃离鞍点（Saddle Point: 鞍点的梯度是0，但是在一些维度上是最高点，在另一些维度上是最低点.）
**Note**: 低维空间的非凸优化问题主要是存在一些局部最优点。基于梯度下降的优化方法会陷入局部最优点，因此**低维空间非凸优化的主要难点是如何选择初始化参数和逃离局部最优点**。深层神经网络的参数非常多，其参数学习是在非常高维空间中的非凸优化问题，其挑战和在低维空间的非凸优化问题有所不同;
**分析**: 在高维空间中，局部最优点要求在每一维度上都是最低点，这种概率非常低。假设网络有10, 000维参数，一个点在某一维上是局部最低点的概率为p，那么在整个参数空间中，局部最优点的概率为$p^{10,000}$，这种可能性非常小。也就是说**高维空间中，大部分梯度为0的点都是鞍点**。基于梯度下降的优化方法会在鞍点附近接近于停滞，同样很难从这些鞍点中逃离。
- 平坦底部: 深层神经网络的参数非常多，并且有一定的**冗余性**，这导致每单个参数对最终损失的影响都比较小，这导致了损失函数在局部最优点附近是一个平坦的区域，称为平坦最小值（Flat Minima）; 并且**在非常大的神经网络中，大部分的局部最小值是相等的**。虽然神经网络有一定概率收敛于比较差的局部最小值，但随着网络规模增加，网络陷入局部最小值的概率大大降低.

**优化算法**:

### 激活函数

**简单的线性堆叠网络,不经过非线性激活函数,并不能学到新的特征,学到的任然是线性关系**,常用的激活函数有:
(1) Sigmoid函数
$$\sigma(x) = \frac{1}{1+e^{-x}}$$
其导数为: $\frac{d\sigma(x)}{dx} = \sigma(x)(1-\sigma(x))$, 优点是求导平滑;
缺点: **容易出现梯度消失现象**(), **输出均值不是zero-centered的**()和**指数运算耗时**.
(2) tanh函数
$$\tanh(x) = \frac{e^{x}- e^{-x}}{e^{x} + e^{-x}}$$
其导数为: $\frac{d\tanh(x)}{dx} = \frac{4}{(e^x + e^{-x})^2}$, tanh函数将输入规范化到[-1,1],并且输出值的平均值为0, 解决了sigmoid函数的non-zero问题,但同样存在**梯度消失和指数运算**问题.
(3) Relu函数
$$Relu(x) = \max(0, x)$$
其导数为$\frac{dRelu(x)}{dx} = 0 , 1,  if x<0 , other$, 收敛速度快于sigmoid和tanh,不存在梯度消失问题,计算复杂度低,不需要指数运算;但输出均值非0,存在**神经元坏死现象**(dead relu),某些神经元可能永远不会被激活,导致相应的参数不会被更新(参数初始化时,采用Xavier初始化方法,使得输入和输出值的方法一致; 学习率太大,导致参数更新变化太大,很容易使得输出值从正值变成负值 - 小的 learning_rate或者使用学习率自动调节的优化算法),Relu不会对数据做规范化处理,使得数据的幅度随模型的层数的增加而不断增大.
(4) Leakly Relu
$$LRelu(x) = \max(0, x) + leak*\min(0, x)$$
其中leak为很小的常数, Leakly提出是为了解决dead relu问题,但其表现并不一定比relu好.

### 参数初始化

设定什么层使用什么初始化方法.

(1). Xavier， kaiming系列

- Xavier均匀分布

```
# xavier初始化方法中服从均匀分布U(−a,a) ，分布的参数a = gain * sqrt(6/fan_in+fan_out)
torch.nn.init.xavier_uniform_(tensor, gain=1)
# gain，增益的大小是依据激活函数类型来设定
torch.nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))
```

- Xavier正态分布

```
# xavier初始化方法中服从正态分布，mean=0,std = gain * sqrt(2/fan_in + fan_out)
torch.nn.init.xavier_normal_(tensor, gain=1)
```

- kaiming均匀分布

```
# 此为均匀分布，U～（-bound, bound）, bound = sqrt(6/(1+a^2)*fan_in)
torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')
```

- kaiming正态分布

```
# 此为0均值的正态分布，N～ (0,std)，其中std = sqrt(2/(1+a^2)*fan_in)
torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')
```

(2) 一般初始化方法

```
torch.nn.init.uniform_(tensor, a=0, b=1)  # 均匀分布
torch.nn.init.normal_(tensor, mean=0, std=1)  # 正态分布
torch.nn.init.constant_(tensor, val)  # 常数分布
torch.nn.init.orthogonal_(tensor, gain=1)  # 正交分布
```

## 正则化

### 泛化能力

泛化能力最直接的定义是**训练数据和真实数据间的差异**;
泛化能力还可以看成模型的**稀疏性**。正如奥斯姆的剪刀指出的，面对不同的解释时，最简单的解释是最好的解释。在机器学习中，具有泛化能力的模型中应该有很多参数是接近0的。而在深度学习中，则是待优化的矩阵应该**对稀疏性有偏好性**。
泛化能力的第三种解释是生成模型中的**高保真能力**。具有泛化能力的模型应在其每个抽象层次具有重构特征的能力。
第四种解释是模型能够有效的忽视琐碎的特征，或者说在无关的变化下都能找到相同的特征。比如CNN就能够忽视其关注特征所在的位置，而capsule网络则能够忽略特征是否旋转。去除掉越来越多的无关特征后，才能保证模型对真正在意的特征的准确生成能力。这和上述的第三点是相辅相成的。
泛化能力还可以看成模型的**信息压缩能力**。这里涉及到解释为什么深度学习有效的一种假说，信息瓶颈（information bottleneck），说的是一个模型对特征进行压缩（降维）的能力越强，其就越更大的可能性做出准确的分类。信息压缩能力可以概括上述的四种关于泛化能力的解释，稀疏的模型因其结构而完成了信息的压缩，生成能力强，泛化误差低的模型因信息压缩而可能，而忽略无关特征是信息压缩的副产品。
理解泛化能力的最后一种角度是**风险最小化**。这是从博弈论的角度来看，泛化能力强的模型能尽可能降低自己在真实环境中遇到意外的风险，因此会在内部产生对未知特征的预警机制，并提前做好应对预案。这是一种很抽象的也不那么精确的解释，但随着技术的进步，人们会找出在该解释下进行模型泛化能力的量化评价方法。
在**机器学习中**，**正则化**很容易理解，不管是**L1还是L2**，都是针对模型中参数过大的问题引入惩罚项。而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。因此在深度学习中, 正则化的方法相对较多, 如L1,L2正则化, dropout/droplink, 数据增强, early stopping, 批量正则化等.

### 正则化技术

正则化技术是保证算法泛化能力的有效工具.

#### 数据增强

数据增强通过向训练数据添加转换或扰动来人工增加训练数据集。数据增强技术如水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转通常应用在视觉表象和图像分类中。

#### L1 和 L2 正则化

L1 正则化向目标函数添加正则化项，以减少参数的值总和；而 L2 正则化中，添加正则化项的目的在于减少参数平方的总和。

#### Dropout

#### early stopping

早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。

**几种新的正则化技术**: [参考](https://zhangbin0917.github.io/2018/09/18/%E5%87%A0%E7%A7%8D%E6%96%B0%E7%9A%84regularization%E6%96%B9%E6%B3%95/)

#### Shake-Shake

The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination.

![shake-shake](/images/blog/shake-shake.png)

#### cutout

Randomly masking out square regions of input during training.

<!-- ![cutout](pic/cutout.jpg) -->

#### mixup与SamplePairing

SamplePairing和mixup是两种一脉相承的图像数据扩增手段，它们看起来很不合理，而操作则非常简单，但结果却非常漂亮：在多个图像分类任务中都表明它们能提高最终分类模型的精度。
<!-- 其中, SamplePairing假设样本$x_a$的标签是$y_a$，那么训练集随机选一个样本$x_b$，让(xa+xb)/2的标签仍然是$y_a$, 当前样本是$x_a$时，随机选到$x_b$，而我们让(xa+xb)/2的标签仍然是$y_a$；而轮到xb时，随机选的时候也可能选到xa，这时候(xa+xb)/2的标签就是xb。所以，看上去不对称，实际上SamplePairing是对称的，SamplePairing可以重新表述为： -->
其中, **SamplePairing是随机选两个样本xa和xb，它们对应的标签为ya,yb，从中随机选一个标签，假设结果为y，那么让(xa+xb)/2的标签为y**.
由此可以发现, 训练充分后, 理论上模型对于(xa+xb)/2的输出应该是ya,yb各占一半，也就是说，如果ya,yb分别代表类别的one hot向量，那么(xa+xb)/2的输出就是(ya+yb)/2.
而mixup是假设U(ε)是某个[0,1]上的随机分布，那么每次我们都随机采样ε∼U(ε)，然后让ε·xa+(1−ε)·xb对应的输出为ε·ya+(1−ε)·yb.
![mixup](/images/blog/mixup.png)

#### ShakeDrop

#### AutoAugment
