---
title: 'VideoQA中的视频数据表征'
date: 2020-05-10
permalink: /posts/2020/05/VideoQA中的视频数据表征/
tags:
  - VideoQA
  - video representation
---

VideoQA问题中的视频数据表征。

<!-- [TOC] -->

## bottom-up的视频表征 (grid-based)

### HGA2020

[Reasoning with Heterogeneous Graph Alignment for Video Question Answering]([.](https://aaai.org/Papers/AAAI/2020GB/AAAI-JiangP.7061.pdf))

- Appearance Features - ResNet  
  $$\mathbf{F}_A = \{\mathbf{a}_i: i\leq L_v,\mathbf{a}_i \in \mathbb{R}^{d_a}\}$$
- Motion Features - C3D    
  $$\mathbf{F}_M = \{\mathbf{m}_i: i\leq L_v,\mathbf{m}_i \in \mathbb{R}^{d_m}\}$$;
  $L_v$是帧数；

- Text Encoding - GloVe-300  
  $$\mathbf{E} = \{\mathbf{e}_i: i\leq L_q,\mathbf{e}_i \in \mathbb{R}^{300}\}$$;
  $L_q$是词嵌入的单词个数。

### HCRN2020

[Hierarchical Conditional Relation Networks for Video Question Answering](https://arxiv.org/pdf/2002.10698.pdf)

将具有$L$帧的视频$\mathcal{V}$等长的划分为$N$个片段$C=(C_1, ..., C_N)$，则每个片段$C_i$的长度为$T=[L/N]$，信息表征为：

- Appearance Features - ResNet  
  $$V_i = \{v_{i,j}\| v_{i,j} \in \mathbb{R}^{2048}\}_{j=1}^T$$
- Motion Features - ResNetXt-101  
  $$f_i \in \mathbb{R}^{2048}$$  
  后接linear feature transformation将二者映射为$d$维向量。

- Text Encoding - GloVe-300  
  $$300 \rightarrow d$$  
  后接biLSTM，输出的两个hidden state拼接成$d$维文本向量。

### QueST2020

[Divide and Conquer: Question-Guided Spatio-Temporal Contextual Attention for Video Question Answering](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-JiangJ.1655.pdf)

- Appearance Features  
  $$\mathbf{V}=[v_1,v_2,...,v_N], v\in \mathbb{R}^{H\times W\times C_v}$$  
  其中$C_v$是特征图的通道数。

- Text Encoding - GloVe-300
  随后输入LSTM得到文本向量。
  
### TSN2019

[Question-Aware Tube-Switch Network for Video Question Answering](https://dl.acm.org/doi/abs/10.1145/3343031.3350969)

- Appearance Features - VGG16  
  $$\mathbf{V}_a=\{\mathbf{v}_1^a,...,\mathbf{v}_N^a \} \in \mathbb{R}^{N\times D}$$
- Motion Features - C3D  
  $$\mathbf{V}_m=\{\mathbf{v}_1^m,...,\mathbf{v}_N^m \} \in \mathbb{R}^{N\times D}$$
- Text Encoding - GloVe300

### HME2019

[Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering](https://arxiv.org/pdf/1904.04357.pdf)

- Appearance Feature - VGG  
  $$\mathbf{f}^a=[\mathbf{f}^a_1, \mathbf{f}^a_2, ..., \mathbf{f}^a_{N_v}]$$
- Motion Feature - C3D  
  $$\mathbf{f}^m=[\mathbf{f}^m_1, \mathbf{f}^m_2, ..., \mathbf{f}^m_{N_v}]$$

- Text Encoding - GloVe300
  
## Relation-Based的视频表征 (region-based)

### HCRN2020

<span style="color:red">虽然只提取了appearance和motion的特征，但是利用采样策略考虑到了relation。</span>

[Hierarchical Conditional Relation Networks for Video Question Answering](https://arxiv.org/pdf/2002.10698.pdf)

将具有$L$帧的视频$\mathcal{V}$等长的划分为$N$个片段$C=(C_1, ..., C_N)$，则每个片段$C_i$的长度为$T=[L/N]$，信息表征为：

- Appearance Features - ResNet   
  $$V_i = \{v_{i,j}\| v_{i,j} \in \mathbb{R}^{2048}\}_{j=1}^T$$
- Motion Features - ResNetXt-101  
  $$f_i \in \mathbb{R}^{2048}$$  
  后接linear feature transformation将二者映射为$d$维向量。

- Text Encoding - GloVe-300  
  $$300 \rightarrow d$$  
  后接biLSTM，输出的两个hidden state拼接成$d$维文本向量。

### L-GCN2020

<span style="color:red">基于帧内目标构建object graph；帧间的关系通过position encoding体现。</span>

[Location-aware Graph Convolutional Networks for Video Question Answering](https://tanmingkui.github.io/files/publications/Location-aware.pdf)

- Object Graph Features

  视频具有$N$帧，每帧有$K$个检测目标，使$$\mathcal{R}=\left\{ \mathbf{o}_{n,k}, \mathbf{b}_{n,k} \right\}_{n=1,k=1}^{n=N,k=K}$$表示目标集合，则$\mathbf{o}$表示目标的RoIAlign特征，$\mathbf{b}$表示目标空间位置。$T=N\times K$表示视频中总的目标个数。具有$M$个节点的图为$\mathcal{G}=(\mathcal{V}, \mathcal{E})$，连接矩阵为 $$ A=\mathbb{R}^{M \times M}$$。

  - Location Encoding  
    **空间编码**：给定视频第$n$帧中空间位置为$\mathbf{b}$，特征为$\mathbf{o}$的目标，使用2层感知机编码$\mathbf{b}$，其中$\mathbf{b}$由目标的bounding box的顶点和宽高表示。
    $\mathbf{d}^s=MLP(\mathbf{b})$  
    **时间编码**：本质就是Transformer中的position encoding。  
    $d_{2j}^t=\sin(n/10000^{2j/d_p})$  
    $d_{2j+1}^t=\cos(n/10000^{2j/d_p})$  
    其中，$d_{i}^t$是时间位置特征$\mathbf{d}^t$的第$i$项。

  最终，图节点可以表示为：$\mathbf{v}=[\mathbf{o};\mathbf{d}^s;\mathbf{d}^t]$

- Frame Features - ResNet152
  
- Text Encoding
  为处理词汇表外的单词和拼写错误的单词，使用character embedding和word embedding。

  - character embedding - Random Initialization  
    $\mathbf{Q}^c \in \mathbb{R}^{k \times c \times d_c}$  
  - word embedding - GloVe-300  
    $\mathbf{Q}^w \in \mathbb{R}^{k \times d_w}$  
  
  其中，$k$为单词个数。随后利用2层highway network编码$\mathbf{Q}^c$和$\mathbf{Q}^w$，其中$\mathbf{Q}^c$先经过卷积$g(\cdot)$处理:  
  $\mathbf{Q}=h(\mathbf{Q}^w, g(\mathbf{Q}^c))$  

  后接biLSTM，输出的两个hidden state拼接成文本向量。

### MiNOR2019

<span style="color:red">基于帧内目标构建object graph；帧间的关系通过在目标location特征表示时加帧的序号和position encoding体现。</span>

[Multi-interaction Network with Object Relation for Video Question Answering](https://dl.acm.org/doi/10.1145/3343031.3351065)

- video object relation  
  - Object Appearance Features  
    $f_A = \{f_1^a, f_2^a,..., f_N^a \}$  
  - Object Location Features  
    $f_L = \{f_1^l, f_2^l,..., f_N^l \}$  
  其中，$N$是视频中检测到的目标的总数；$f_L$是5D向量表示的时空位置：  
    $f_n^l=(x_n, y_n, w_n, h_n, t_n)$  
    其中$(x_n, y_n, w_n, h_n)$是目标是bbox的位置，$t_n$是目标所在的帧数。  
  给定两目标的时空位置特征$f_n^l$和$f_m^l$，则二者间的关系计算：  
  $X_{mn}=\log(\frac{|x_m - x_n|}{w_n})$, $Y_{mn}=\log(\frac{|y_m - y_n|}{h_n})$  
  $W_{mn}=\log(\frac{|w_m|}{w_n})$, $H_{mn}=\log(\frac{|h_m|}{h_n})$  
  $T_{mn}=\log(\frac{|t_m|}{t_n})$  
  当$t_m > t_n$时，$T_mn$为正，否则为负。  
  随后，利用position encoding编码5D的关系向量$(X_{mn}, Y_{mn}, W_{mn}, H_{mn}, T_{mn})^{\mathrm{T}}$，最后concatenate这五个嵌入向量为一个$f_{mn}^r$。那么目标m和n之间时空关系的权重为：  
  $\omega_{mn}^r=\max \{ 0, W_r \cdot f_{mn}^r \}$  
  --- ReLU激活的目的是避免模型学到某种统一的关系。

### MSAN2020

[Modality Shifting Attention Network for Multi-modal Video Question Answering](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Modality_Shifting_Attention_Network_for_Multi-Modal_Video_Question_Answering_CVPR_2020_paper.pdf)

- Object Feature
  以三帧每秒的帧率采样视频，然后用在VG数据集上训练的Faster-RCNN检测目标和属性。
  并用Kinetics上训练的I3D识别视频片段的action label，并选top-5的label组成action label。


## 总结

VideoQA中video表征发展的大趋势是grid-based $\rightarrow$ region-based。region-based的方法能更好的表征高级的语义关系和内容，但表征能力依赖于视频目标检测和关系提取算法的性能，也无法实现端到端的训练。CVPR2020 Facebook AI的论文[12-in-1](https://research.fb.com/wp-content/uploads/2020/06/12-in-1-Multi-Task-Vision-and-Language-Representation-Learning-v2.pdf)和比赛表明grid feature能取得和region feature一样甚至更好的性能。
